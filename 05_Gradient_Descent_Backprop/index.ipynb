{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0x05 Gradient descent and backpropagation\n",
    "\n",
    "In this tutorial we will cover the implementations and how-tos for interacting\n",
    "with the backpropagation engine of PyTorch.\n",
    "\n",
    "PyTorch is the go-to library for deep learning in Python especially if you are building a custom model on your own.\n",
    "You will be very likely be using PyTorch when you are doing your research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **NOTE**: \n",
    ">\n",
    "> We assume you have already learnt the fundamentals of derivatives and gradients.\n",
    ">\n",
    "> If you need a quick recap, check out this explanation [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#optional-reading-vector-calculus-using-autograd) by the PyTorch team.\n",
    ">\n",
    "> Another suggested deep walkthrough is this [3blue1brown video](https://www.youtube.com/watch?v=tIeHLnjs5U8) on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Backpropagation\n",
    "\n",
    "One of PyTorch tensors' biggest difference with NumPy arrays is that they can track gradients.\n",
    "\n",
    "Two important ways to interact with it are `requires_grad` and the `.grad` property.\n",
    "Let us see it with a simple example.\n",
    "\n",
    "Consider this formula:\n",
    "$$\n",
    "y = w_1x^2 + w_2x + b\n",
    "$$\n",
    "\n",
    "where $x$ is the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **THINKING**\n",
    ">\n",
    "> - What is $\\frac{\\partial y}{\\partial w_1}$, $\\frac{\\partial y}{\\partial w_2}$, $\\frac{\\partial y}{\\partial b}$? Compute by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove your computation, let us implement this formula in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0])\n",
    "\n",
    "# Identify the parameters that you need to compute gradients for\n",
    "# and set requires_grad=True\n",
    "w1 = torch.tensor([1.0], requires_grad=True)\n",
    "w2 = torch.tensor([3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0], requires_grad=True)\n",
    "# You will see why we used an intermediate variable z here.\n",
    "z = w1 * torch.pow(x, 2) + w2 * x\n",
    "y = z + b\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute backpropagation\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n",
      "tensor([2.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# Gradient w.r.t. w1, w2, and b are stored in .grad of the tensors\n",
    "print(w1.grad)  # dy/dw1\n",
    "print(w2.grad)  # dy/dw2\n",
    "print(b.grad)    # dy/db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your computation correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you may want to also get $\\frac{dy}{dz}$ from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g1/zn3hm6l93hn5t3wcpy29ty1w0000gn/T/ipykernel_60416/385971827.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_626md4ifaq/croot/libtorch_1738971453702/work/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(z.grad)  # dy/dz\n"
     ]
    }
   ],
   "source": [
    "print(z.grad)  # dy/dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no ðŸ˜±, we cannot do it!\n",
    "\n",
    "This is because PyTorch does not update the gradients on **non-leaf** tensors. This makes sense because model parameters are leaf tensors. If you really **DO** want to compute the gradients of non-leaf tensors for specific use cases, you can use [`.retain_grad()`](https://pytorch.org/docs/stable/generated/torch.Tensor.retain_grad.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“š **EXERCISE**\n",
    ">\n",
    "> - Define an expression on your own and get the gradients using backpropagation.\n",
    "> - Currently, our `y` is a scalar. Although loss is usually scalar in deep learning, what if we have a vector as `y`? How do we compute the gradients in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Toggling gradient tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient descent and optimizing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-handson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
